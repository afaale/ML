{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6qkwEwE8AH0wPGn1dKKYd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afaale/ML/blob/ML/Bert_transfer_lr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **model1**"
      ],
      "metadata": {
        "id": "fpYDQMY5mMLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "qmvNBP7VCXz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "y5-MsmHbRlyO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYTaH8xSCNxR"
      },
      "outputs": [],
      "source": [
        "unmasker = pipeline('fill-mask', model='bert-base-cased')\n",
        "unmasker(\"King is a [MASK].\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"cola\")\n",
        "dataset_train = dataset[\"train\"]  \n",
        "dataset_test = dataset['test']\n"
      ],
      "metadata": {
        "id": "Ip_0RCN2RJ41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test"
      ],
      "metadata": {
        "id": "9lUDvv65X-iV",
        "outputId": "bf488db4-f687-447c-d473-0df0b09e06fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'label', 'idx'],\n",
              "    num_rows: 1063\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and compile our model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "fWiRWZ9rHwUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "\n",
        "def tokenize_dataset(data):\n",
        "    # Keys of the returned dictionary will be added to the dataset as columns\n",
        "    return tokenizer(data[\"sentence\"])\n",
        "\n",
        "\n",
        "dataset_train = dataset_train.map(tokenize_dataset)\n",
        "dataset_test = dataset_test.map(tokenize_dataset)"
      ],
      "metadata": {
        "id": "c03lJHICaBd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train[0]"
      ],
      "metadata": {
        "id": "HY8iUatdfHc5",
        "outputId": "8a4bf78d-d877-43b7-9479-eb4006f8db63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
              " 'label': 1,\n",
              " 'idx': 0,\n",
              " 'input_ids': [101,\n",
              "  2256,\n",
              "  2814,\n",
              "  2180,\n",
              "  1005,\n",
              "  1056,\n",
              "  4965,\n",
              "  2023,\n",
              "  4106,\n",
              "  1010,\n",
              "  2292,\n",
              "  2894,\n",
              "  1996,\n",
              "  2279,\n",
              "  2028,\n",
              "  2057,\n",
              "  16599,\n",
              "  1012,\n",
              "  102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dataset = model.prepare_tf_dataset(dataset_train, batch_size=16, shuffle=True, tokenizer=tokenizer)\n",
        "model.compile(optimizer=Adam(3e-5), metrics=['accuracy'])  # No loss argument!\n",
        "\n",
        "history = model.fit(tf_dataset, epochs=1)"
      ],
      "metadata": {
        "id": "xZqtQ42SWDGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\",  \"Rocketknight1/distilbert-base-uncased-finetuned-squad\", framework=\"tf\")\n"
      ],
      "metadata": {
        "id": "zxdMzGw8Zz6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"The dominant sequence transduction models are based on complex recurrent or convolutional \n",
        "neural networks in an encoder-decoder configuration. The best performing models also connect the encoder\n",
        "and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \n",
        "based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on \n",
        "two machine translation tasks show these models to be superior in quality while being more parallelizable \n",
        "and requiring significantly less time to train.\"\"\"\n",
        "question = \"What are Transformers?\"\n",
        "question_answerer(context, question, )"
      ],
      "metadata": {
        "id": "uLiE8bkJdAyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 2"
      ],
      "metadata": {
        "id": "OvHAFQddkPHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "Sp3ZQv1CmTXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "d3MCYzRcwc_L"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"glue\", \"cola\")\n",
        "dataset_train = dataset[\"train\"]  \n",
        "dataset_test = dataset['test']\n"
      ],
      "metadata": {
        "id": "Uim1XaulooUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data = tokenizer(dataset_train[\"sentence\"], return_tensors=\"np\", padding=True)\n",
        "tokenized_data = dict(tokenized_data)\n",
        "labels = np.array(dataset_train[\"label\"])  # Label is already an array of 0 and 1\n"
      ],
      "metadata": {
        "id": "HgZekh5Etisr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and compile our model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "# Lower learning rates are often better for fine-tuning transformers\n",
        "model.compile(optimizer=Adam(3e-5))  # No loss argument!"
      ],
      "metadata": {
        "id": "utT98-_6xpax",
        "outputId": "d926a0e3-c5b9-417e-b907-e1ae2d894b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_data_test = dict(tokenizer(dataset_test[\"sentence\"], return_tensors=\"np\", padding=True))\n",
        "labels_test = np.array(dataset_test[\"label\"])  # Label is already an array of 0 and 1\n",
        "\n",
        "\n",
        "model.evaluate(tokenized_data_test)"
      ],
      "metadata": {
        "id": "92F5v1pByxSy",
        "outputId": "ab404b1a-fe08-46ab-8574-e3d823b1e39a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34/34 [==============================] - 9s 9ms/step - loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model3"
      ],
      "metadata": {
        "id": "7wMCwHhxyz7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
        "\n",
        "# You can, of course, use your own username and model name here \n",
        "# once you've pushed your model using the code above!\n",
        "checkpoint = \"Rocketknight1/distilgpt2-finetuned-wikitext2\"\n",
        "model = TFAutoModelForCausalLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "TPwCESK3kURD",
        "outputId": "d5364c14-5f9d-4d96-d407-9ebfa50fc28c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at Rocketknight1/distilgpt2-finetuned-wikitext2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_dataset(data):\n",
        "    # Keys of the returned dictionary will be added to the dataset as columns\n",
        "    return tokenizer(data[\"sentence\"])\n",
        "\n",
        "\n",
        "dataset_train = dataset_train.map(tokenize_dataset)\n",
        "dataset_test = dataset_test.map(tokenize_dataset)"
      ],
      "metadata": {
        "id": "wHTdhLKHp81k",
        "outputId": "15610153-7915-4555-e2af-7de7bec8dd1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-91243d67d09e299f.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f870469c002a14eb.arrow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(tokenized_data, labels)\n"
      ],
      "metadata": {
        "id": "cnwffVlOraOi",
        "outputId": "0759ca6f-76a3-4606-82af-00a4de780931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cb51c9f23fdf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt(test_sentence):\n",
        "    tokenized = tokenizer(test_sentence, return_tensors=\"np\")\n",
        "    outputs = model.generate(**tokenized, max_length=32)\n",
        "    print('\\n')\n",
        "    print(tokenizer.decode(outputs[0]))\n",
        "\n",
        "test_sentence = \"A confusion matrix is\"\n",
        "prompt(test_sentence)"
      ],
      "metadata": {
        "id": "8udAybvFkxxs",
        "outputId": "459af92e-c7b8-41f7-c5c3-8ac09a29031e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "A confusion matrix is a matrix of two elements. The first element is the first element, the second element is the second element. The first element is the second element. The second element is the third element. The third element is the fourth element. The fourth element is the fourth element. The fourth element is the fourth\n"
          ]
        }
      ]
    }
  ]
}